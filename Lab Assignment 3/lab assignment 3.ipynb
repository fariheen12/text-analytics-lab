{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c05dd50-9dac-4e2d-9c5c-410d58ef2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab Assignment 3\n",
    "# MUHAMMAD FARIHEEN BIN ABD RAHIM (SW01082818)\n",
    "# MUHAMMAD ADEEB BIN ABDULLAH (SW01082814)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57747b4-deab-4723-8000-14d6e323e5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('news_dataset.csv')\n",
    "\n",
    "# Remove null values\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize each token\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Show sample\n",
    "df[['text', 'processed_text']].head()\n",
    "\n",
    "df.to_csv(\"news_dataset_processed.csv\", index=False)\n",
    "# save in new file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc069694-0ed3-4671-a078-9cd9588e56b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>[wondering, anyone, could, enlighten, car, saw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I recently posted an article asking what kind ...</td>\n",
       "      <td>[recently, posted, article, asking, kind, rate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nIt depends on your priorities.  A lot of peo...</td>\n",
       "      <td>[depends, priority, lot, people, put, higher, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>[excellent, automatic, found, subaru, legacy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>: Ford and his automobile.  I need information...</td>\n",
       "      <td>[ford, automobile, need, information, whether,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  I was wondering if anyone out there could enli...   \n",
       "1  I recently posted an article asking what kind ...   \n",
       "2  \\nIt depends on your priorities.  A lot of peo...   \n",
       "3  an excellent automatic can be found in the sub...   \n",
       "4  : Ford and his automobile.  I need information...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [wondering, anyone, could, enlighten, car, saw...  \n",
       "1  [recently, posted, article, asking, kind, rate...  \n",
       "2  [depends, priority, lot, people, put, higher, ...  \n",
       "3  [excellent, automatic, found, subaru, legacy, ...  \n",
       "4  [ford, automobile, need, information, whether,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample\n",
    "df[['text', 'processed_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca6cc7e-0a5b-4def-813b-91091b60b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "\n",
      "Topic #0:\n",
      "- people (0.0118)\n",
      "- would (0.0112)\n",
      "- one (0.0098)\n",
      "- think (0.0065)\n",
      "- say (0.0061)\n",
      "- know (0.0058)\n",
      "- right (0.0051)\n",
      "- god (0.0051)\n",
      "- time (0.0048)\n",
      "- government (0.0047)\n",
      "\n",
      "Topic #1:\n",
      "- max (0.0270)\n",
      "- president (0.0118)\n",
      "- new (0.0087)\n",
      "- american (0.0065)\n",
      "- year (0.0060)\n",
      "- national (0.0058)\n",
      "- program (0.0057)\n",
      "- university (0.0053)\n",
      "- administration (0.0052)\n",
      "- state (0.0051)\n",
      "\n",
      "Topic #2:\n",
      "- key (0.0143)\n",
      "- use (0.0104)\n",
      "- file (0.0098)\n",
      "- system (0.0095)\n",
      "- chip (0.0077)\n",
      "- encryption (0.0067)\n",
      "- one (0.0061)\n",
      "- window (0.0060)\n",
      "- program (0.0057)\n",
      "- information (0.0055)\n",
      "\n",
      "Topic #3:\n",
      "- would (0.0102)\n",
      "- one (0.0101)\n",
      "- get (0.0091)\n",
      "- like (0.0084)\n",
      "- game (0.0079)\n",
      "- year (0.0070)\n",
      "- good (0.0067)\n",
      "- time (0.0061)\n",
      "- know (0.0061)\n",
      "- think (0.0057)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Load preprocessed CSV\n",
    "df = pd.read_csv(\"news_dataset_processed.csv\")\n",
    "df['processed_text'] = df['processed_text'].apply(ast.literal_eval)\n",
    "\n",
    "# Optional cleanup: remove short or junk tokens (if needed)\n",
    "df['processed_text'] = df['processed_text'].apply(lambda tokens: [t for t in tokens if t.isalpha() and len(t) > 2])\n",
    "\n",
    "# Assign to list\n",
    "preprocessed_documents = df['processed_text'].tolist()\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=4,\n",
    "                     passes=15,\n",
    "                     random_state=42)\n",
    "\n",
    "# Show top 10 terms per topic\n",
    "print(\"Top Terms for Each Topic:\\n\")\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    print(f\"Topic #{topic_id}:\")\n",
    "    terms = lda_model.show_topic(topic_id, topn=10)\n",
    "    for term, weight in terms:\n",
    "        print(f\"- {term} ({weight:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006d70a6-86d0-4bce-9f21-56be9a7bde4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.5078\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Evaluate the coherence score\n",
    "coherence_model = CoherenceModel(model=lda_model, \n",
    "                                  texts=preprocessed_documents, \n",
    "                                  dictionary=dictionary, \n",
    "                                  coherence='c_v')\n",
    "\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31e0eeec-746d-4136-95af-eb58765bfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LDA model achieved a coherence score of approximately 0.5078, indicating a moderate level of interpretability.\n",
    "# This means the topics generated are generally meaningful and understandable, though there may still be some overlap.\n",
    "# It's a decent result for exploratory analysis and shows the model has captured useful themes in the data.\n",
    "# Thereâ€™s still room for improvement, and the coherence could be increased by tuning topic numbers or refining preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
