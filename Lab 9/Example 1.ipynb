{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539af2c2-4a5c-4168-8cdd-0f88207132ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\60139\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\60139\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\60139\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Download NLTK Resources (only once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a28b505-6af6-49c4-a4a8-28d7a58639ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the Data\n",
    "documents = [\n",
    "    \"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
    "    \"Rafael Nadal Is Out of the Australian Open\",\n",
    "    \"Biden Announces Virus Measures\",\n",
    "    \"Biden's Virus Plans Meet Reality\",\n",
    "    \"Where Biden's Virus Plan Stands\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a50207-1c48-4435-bcf7-4734c76e98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess the Data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Remove non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
    "    return tokens\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f40716-e23a-499f-8726-91e5272b8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a Document-Term Matrix\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52504e09-45f2-4d68-8989-e867048658e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Run LDA\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d0e11f2-d133-462d-b54e-0cab1aff960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                             Article  Topic\n",
      "0  Rafael Nadal Joins Roger Federer in Missing U....      1\n",
      "1         Rafael Nadal Is Out of the Australian Open      1\n",
      "2                     Biden Announces Virus Measures      0\n",
      "3                   Biden's Virus Plans Meet Reality      0\n",
      "4                    Where Biden's Virus Plan Stands      0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Interpret Results\n",
    "# Empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "# Iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # Convert document to bag-of-words\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # Get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # Determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # Append dominant topic to list\n",
    "    article_labels.append(dominant_topic)\n",
    "\n",
    "# Create DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2c667bf-2b06-4992-aae1-5265a8cb7293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"virus\" (weight: 0.166)\n",
      "- \"biden\" (weight: 0.166)\n",
      "- \"plan\" (weight: 0.119)\n",
      "- \"reality\" (weight: 0.071)\n",
      "- \"meet\" (weight: 0.071)\n",
      "- \"announces\" (weight: 0.071)\n",
      "- \"measure\" (weight: 0.071)\n",
      "- \"stand\" (weight: 0.071)\n",
      "- \"rafael\" (weight: 0.024)\n",
      "- \"open\" (weight: 0.024)\n",
      "\n",
      "Topic 1:\n",
      "- \"nadal\" (weight: 0.131)\n",
      "- \"open\" (weight: 0.131)\n",
      "- \"rafael\" (weight: 0.131)\n",
      "- \"missing\" (weight: 0.079)\n",
      "- \"federer\" (weight: 0.079)\n",
      "- \"roger\" (weight: 0.079)\n",
      "- \"join\" (weight: 0.079)\n",
      "- \"australian\" (weight: 0.078)\n",
      "- \"stand\" (weight: 0.027)\n",
      "- \"plan\" (weight: 0.027)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top terms for each topic\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1ae80-9482-4672-a1da-4351f04d65c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
