{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1778f9a-ad2a-4857-a3b5-f7bda75e1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 0\n",
      "Empty or Whitespace-Only Reviews: 0\n",
      "Special Characters & Punctuation: 42\n",
      "Duplicate Reviews: 0\n",
      "Numeric Values: 4\n",
      "URLs: 0\n",
      "HTML Tags: 0\n",
      "Emojis: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdade\\AppData\\Local\\Temp\\ipykernel_440\\2243950488.py:28: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  html_tags = df[\"Review\"].apply(lambda x: bool(BeautifulSoup(x, \"html.parser\").find())).sum()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"UNITENReview.csv\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df[\"Review\"].isnull().sum()\n",
    "\n",
    "# Check for empty or whitespace-only reviews\n",
    "empty_reviews = (df[\"Review\"].str.strip() == \"\").sum()\n",
    "\n",
    "# Check for special characters and punctuation\n",
    "special_chars = df[\"Review\"].str.contains(r\"[^\\w\\s]\", regex=True).sum()\n",
    "\n",
    "# Check for duplicate reviews\n",
    "duplicate_reviews = df[\"Review\"].duplicated().sum()\n",
    "\n",
    "# Check for numeric values\n",
    "numeric_values = df[\"Review\"].str.contains(r\"\\d\", regex=True).sum()\n",
    "\n",
    "# Check for URLs\n",
    "url_pattern = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "url_count = df[\"Review\"].str.contains(url_pattern, regex=True).sum()\n",
    "\n",
    "# Check for HTML tags\n",
    "html_tags = df[\"Review\"].apply(lambda x: bool(BeautifulSoup(x, \"html.parser\").find())).sum()\n",
    "\n",
    "# Check for emojis\n",
    "emoji_pattern = r\"[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F700-\\U0001F77F]|[\\U0001F780-\\U0001F7FF]|[\\U0001F800-\\U0001F8FF]|[\\U0001F900-\\U0001F9FF]|[\\U0001FA00-\\U0001FA6F]|[\\U0001FA70-\\U0001FAFF]|[\\U00002702-\\U000027B0]|[\\U000024C2-\\U0001F251]\"\n",
    "emoji_count = df[\"Review\"].str.contains(emoji_pattern, regex=True).sum()\n",
    "\n",
    "# Display results\n",
    "print(f\"Missing Values: {missing_values}\")\n",
    "print(f\"Empty or Whitespace-Only Reviews: {empty_reviews}\")\n",
    "print(f\"Special Characters & Punctuation: {special_chars}\")\n",
    "print(f\"Duplicate Reviews: {duplicate_reviews}\")\n",
    "print(f\"Numeric Values: {numeric_values}\")\n",
    "print(f\"URLs: {url_count}\")\n",
    "print(f\"HTML Tags: {html_tags}\")\n",
    "print(f\"Emojis: {emoji_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29474ff1-8657-407e-bb51-3e833c8e591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mdade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "C:\\Users\\mdade\\AppData\\Local\\Temp\\ipykernel_1604\\2389734278.py:105: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                         Review  \\\n",
      "0                                                                                                                                                                                                                                                                                                          Im happy with uniten actually, even the people are W   \n",
      "1                                                                                                                                                                                                                                                                                        I’m having a pretty good time here, happy to meet all of the W people.   \n",
      "2                                                                                                                                                                                                                                                                                                                   a very neutral place in terms of everything   \n",
      "3                                                                                                                                                                                                   I would say Uniten it's  a good university  but there is some issue need to be improved such as transportation,wifi networks and other facilities  as well.   \n",
      "4   UNITEN is well-regarded, particularly for its strong engineering, computer science, and business programs. It has a solid reputation in Malaysia, especially in energy-related fields. The negative part is the facilities such as the swimming pool are close since my first year till 3 year now and there are limited parking so it's make hard to find.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                Cleaned  \n",
      "0                                                                                                                                                                                                                    [im, happy, unite, actually, even, people, winner]  \n",
      "1                                                                                                                                                                                                            [i, ’, m, pretty, good, time, happy, meet, winner, people]  \n",
      "2                                                                                                                                                                                                                                    [neutral, place, term, everything]  \n",
      "3                                                                                                                                                             [would, say, united, good, university, issue, need, improve, transportationwifi, network, facility, well]  \n",
      "4  [united, wellregarded, particularly, strong, engineering, computer, science, business, program, solid, reputation, malaysia, especially, energyrelated, field, negative, part, facility, swim, pool, close, since, first, year, year, limit, park, make, hard, find]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')                    # For lemmatization\n",
    "nltk.download('omw-1.4')                     # WordNet lexical database\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # For POS tagging\n",
    "nltk.download('punkt_tab')                       # For tokenization\n",
    "\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Dictionary of slang words and their replacements\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"w\": \"winner\",\n",
    "    \"uniten\": \"uniten\",\n",
    "    \"till\": \"until\"\n",
    "}\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"I'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\"\n",
    "}\n",
    "\n",
    "# Remove any URLs that start with \"http\" or \"www\" from the text\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "# extracts only the text, removing all HTML tags\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# replace emoji with ''\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Replace internet slang/chat words\n",
    "def replace_slang(text):\n",
    "    # Create a list of escaped slang words\n",
    "    escaped_slang_words = []  # Empty list to store escaped slang words\n",
    "\n",
    "    for word in slang_dict.keys():\n",
    "        escaped_word = re.escape(word)  # Ensure special characters are escaped\n",
    "        escaped_slang_words.append(escaped_word)  # Add to list\n",
    "\n",
    "    # Join the words using '|'\n",
    "    slang_pattern = r'\\b(' + '|'.join(escaped_slang_words) + r')\\b'\n",
    "\n",
    "    # Define a replacement function\n",
    "    def replace_match(match):\n",
    "        slang_word = match.group(0)  # Extract matched slang word\n",
    "        return slang_dict[slang_word.lower()]  # Replace with full form\n",
    "\n",
    "    # Use regex to replace slang words with full forms\n",
    "    replaced_text = re.sub(slang_pattern, replace_match, text, flags=re.IGNORECASE)\n",
    "\n",
    "    return replaced_text\n",
    "\n",
    "# Function to expand contractions\n",
    "# Build the regex pattern for contractions\n",
    "escaped_contractions = []  # List to store escaped contractions\n",
    "\n",
    "for contraction in contractions_dict.keys():\n",
    "    escaped_contraction = re.escape(contraction)  # Escape special characters (e.g., apostrophes)\n",
    "    escaped_contractions.append(escaped_contraction)  # Add to list\n",
    "\n",
    "# Join the escaped contractions with '|'\n",
    "joined_contractions = \"|\".join(escaped_contractions)\n",
    "\n",
    "# Create a regex pattern with word boundaries (\\b)\n",
    "contractions_pattern = r'\\b(' + joined_contractions + r')\\b'\n",
    "\n",
    "# Compile the regex\n",
    "compiled_pattern = re.compile(contractions_pattern, flags=re.IGNORECASE)\n",
    "\n",
    "# Define a function to replace contractions\n",
    "def replace_contractions(text):\n",
    "    # Function to handle each match found\n",
    "    def replace_match(match):\n",
    "        matched_word = match.group(0)  # Extract matched contraction\n",
    "        lower_matched_word = matched_word.lower()  # Convert to lowercase\n",
    "        expanded_form = contractions_dict[lower_matched_word]  # Get full form from dictionary\n",
    "        return expanded_form  # Return the expanded form\n",
    "\n",
    "    # Apply regex substitution\n",
    "    expanded_text = compiled_pattern.sub(replace_match, text)\n",
    "\n",
    "    return expanded_text  # Return modified text\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Function to correct spelling using AutoCorrect\n",
    "def correct_spelling(text):\n",
    "    return spell(text)  # Apply correction\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):  # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):  # Verb\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):  # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):  # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# Function to lemmatize text with POS tagging\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):  # Ensure input is a string\n",
    "        return \"\"\n",
    "\n",
    "    words = word_tokenize(text)  # Tokenize text into words\n",
    "    pos_tags = pos_tag(words)  # Get POS tags\n",
    "    \n",
    "    # Lemmatize each word with its correct POS tag\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)  # Join words back into a sentence\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):  # Ensure the input is a string\n",
    "        return []\n",
    "    return word_tokenize(text)  # Tokenize text into words\n",
    "\n",
    "# Function to apply all preprocessing steps\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()           # Step 1: Lowercasing\n",
    "    text = remove_urls(text)               # Step 2: Remove URLs\n",
    "    text = remove_html(text)               # Step 3: Remove HTML tags\n",
    "    text = remove_emojis(text)             # Step 4: Remove Emojis\n",
    "    text = replace_slang(text)             # Step 5: Replace Slang\n",
    "    text = replace_contractions(text)       # Step 6: Expand Contractions\n",
    "    text = remove_punctuation(text)        # Step 7: Remove Punctuation\n",
    "    text = remove_numbers(text)            # Step 8: Remove Numbers\n",
    "    text = correct_spelling(text)          # Step 9: Correct Spelling\n",
    "    text = remove_stopwords(text)          # Step 10: Remove Stopwords\n",
    "    text = lemmatize_text(text)            # Step 11: Lemmatization\n",
    "    text = tokenize_text(text)             # Step 12: Tokenization\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"UNITENReview.csv\")  \n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "df[\"Cleaned\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df[[\"Review\", \"Cleaned\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0edba4b2-754d-4d13-9f27-4d9bdd498cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "df.to_csv(\"UNITENReview_Cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604a3fa-8401-42e2-89fb-7d10ab9ae816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
