{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff186574-e786-4629-ae89-6d152dd0aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      "  investig\n",
      "  protocol\n",
      "  malfunct\n",
      "  sinc\n",
      "  custom\n",
      "  altern\n",
      "  illog\n",
      "  certain\n",
      "  follow\n",
      "  predecessor\n",
      "\n",
      "Cluster 1:\n",
      "  internet\n",
      "  day\n",
      "  comcast\n",
      "  cabl\n",
      "  tech\n",
      "  time\n",
      "  servic\n",
      "  set\n",
      "  im\n",
      "  secur\n",
      "\n",
      "Cluster 2:\n",
      "  rude\n",
      "  servic\n",
      "  second\n",
      "  bill\n",
      "  box\n",
      "  comcast\n",
      "  rep\n",
      "  joke\n",
      "  pass\n",
      "  resolv\n",
      "\n",
      "Cluster 3:\n",
      "  speed\n",
      "  mbp\n",
      "  contract\n",
      "  custom\n",
      "  pay\n",
      "  say\n",
      "  internet\n",
      "  servic\n",
      "  call\n",
      "  blast\n",
      "\n",
      "Cluster 4:\n",
      "  xfiniti\n",
      "  would\n",
      "  contract\n",
      "  state\n",
      "  call\n",
      "  cancel\n",
      "  sign\n",
      "  servic\n",
      "  store\n",
      "  area\n",
      "\n",
      "Purity: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"customer_complaints_1.csv\")\n",
    "\n",
    "# List of common stopwords to remove\n",
    "basic_stopwords = {\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his',\n",
    "    'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',\n",
    "    'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
    "    'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "    'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    "    'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "    'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
    "    'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "    's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "}\n",
    "\n",
    "# Prepare stemming tool\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Clean and preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in basic_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning to the text column\n",
    "df['processed_text'] = df['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Convert text into TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Group text into clusters\n",
    "k = 5\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "km.fit(X)\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Show top 10 keywords for each cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\"  {terms[ind]}\")\n",
    "    print()\n",
    "\n",
    "# Calculate and print purity (dummy since no real labels)\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = Counter(y_pred)\n",
    "purity = sum(cluster_label_counts.values()) / total_samples\n",
    "print(f\"Purity: {purity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe29cc9-39a2-4368-9421-d6d31b6ed985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
