{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea8129e-6fab-4f7a-b9cd-2461848bfb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TF-IDF with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a798f15c-6068-4943-ac36-3f75da73efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\60139\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " books\n",
      " like\n",
      " read\n",
      " movies\n",
      " watch\n",
      " camping\n",
      " concerts\n",
      " enjoy\n",
      " football\n",
      " games\n",
      "\n",
      "Cluster 1:\n",
      " love\n",
      " playing\n",
      " football\n",
      " weekends\n",
      " enjoy\n",
      " camping\n",
      " mountains\n",
      " hiking\n",
      " sports\n",
      " music\n",
      "\n",
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Dataset\n",
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n",
    "\n",
    "# Preprocess each document\n",
    "dataset_clean = [preprocess(doc) for doc in dataset]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset_clean)\n",
    "\n",
    "# KMeans Clustering\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Show results\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()\n",
    "\n",
    "# Purity Calculation\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67766e30-5995-4e52-8990-7166c30639ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Word2Vec with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57e5de2-0bce-48c5-b66d-a0fdbbaef6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\60139\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    0\n",
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Dataset\n",
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n",
    "\n",
    "# Preprocess dataset\n",
    "tokenized_dataset = [preprocess(doc) for doc in dataset]\n",
    "\n",
    "# Word2Vec Model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Document embeddings (mean of word vectors)\n",
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc if word in word2vec_model.wv], axis=0)\n",
    "              for doc in tokenized_dataset])\n",
    "\n",
    "# KMeans Clustering\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Show results\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Purity Calculation\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b9e63-f126-4e4e-b85a-b8db3f43f3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
